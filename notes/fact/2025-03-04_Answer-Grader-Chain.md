---
date: 2025-03-04
type: fact
aliases:
  -
hubs:
  - "[[langgraph]]"
---

# Answer Grader Chain

In this topic, we need to establish a chain to determine whether the answer generated by LLM is truly used to answer the user's question.

This chain should be similar to the chain of [[2025-03-04_Grade-Hallucinations-Chain|Grade Hallucinations Chain]], just replacing documents with user questions.

```sh
 tree
.
├── graph
│   ├── chains
│   │   ├── answer_grader.py # Answer Grader Chain
│   │   ├── generation.py
│   │   ├── hallucination_grader.py
│   │   ├── __init__.py
│   │   ├── retrieval_grader.py
│   │   └── tests
│   │       ├── __init__.py
│   │       └── test_chains.py
│   ├── consts.py
│   ├── graph.py
│   ├── __init__.py
│   ├── nodes
│   │   ├── generate.py
│   │   ├── grade_documents.py
│   │   ├── __init__.py
│   │   ├── retrieve.py
│   │   └── web_search.py
│   └── state.py
├── ingestion.py
├── main.py
├── Pipfile
└── Pipfile.lock

```

We can copy the `hallucination_grader.py` file and modify it to create the `answer_grader.py` file.

```py
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI


llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

class GradeAnswer(BaseModel): # instead of GradeHallucination, rename to GradeAnswer to fit the context

    binary_score: bool = Field(
        description="Answer address the question, True or False." # modify the description
    )


structured_llm_grader = llm.with_structured_output(GradeAnswer)
#                                                  ^^^^^^^^^^^


answer_prompt= ChatPromptTemplate.from_messages( # instead of hallucination_prompt, rename to answer_prompt
    [
        (
            "system",
            # modify the prompt
            """You are a grader assessing whether an answer addresses / resolves a question. \nGive a binary score True or False. True means that the answer resolves the question."""
            ),
        # modify the prompt, instead of {documents}, here should be {question}
        ("human", "User question: \n\n {question} \n\n LLM generation: {generation}")
    ]
)

answer_grader = answer_prompt | structured_llm_grader # instead of hallucination_grader, rename to answer_grader
#               ^^^^^^

```

Write tests for the answer grader chain.

```py
...

from graph.chains.answer_grader import answer_grader, GradeAnswer

...

def test_answer_grader_true() -> None:
    question = "agent memory"
    docs = retriever.invoke(question)
    generation = generation_chain.invoke({"context": docs, "question": question})
    res = answer_grader.invoke(
        {
            "question": question,
            "generation": generation
        }
    )

    if isinstance(res, GradeAnswer):
        assert res.binary_score
    else:
        assert False


def test_answer_grader_false() -> None:
    question = "agent memory"
    res = answer_grader.invoke(
        {
            "question": question,
            "generation": "this is how to make a cake"
        }
    )

    if isinstance(res, GradeAnswer):
        assert not res.binary_score
    else:
        assert False

```

Run the tests.

```sh
 pytest . -s -v
================================================================== test session starts ===================================================================
platform linux -- Python 3.13.1, pytest-8.3.4, pluggy-1.5.0 -- /home/matt/.local/share/virtualenvs/langgraph-course-uhZ6dcGU/bin/python
cachedir: .pytest_cache
rootdir: /home/matt/Projects/langgraph-course
plugins: langsmith-0.3.11, anyio-4.8.0
collected 2 items                                                                                                                                        

graph/chains/tests/test_chains.py::test_answer_grader_true PASSED
graph/chains/tests/test_chains.py::test_answer_grader_false PASSED

```



